{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3883792,"sourceType":"datasetVersion","datasetId":2307840},{"sourceId":5997654,"sourceType":"datasetVersion","datasetId":3436199}],"dockerImageVersionId":30512,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install ultralytics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T08:05:43.575925Z","iopub.execute_input":"2023-07-01T08:05:43.5763Z","iopub.status.idle":"2023-07-01T08:05:58.51354Z","shell.execute_reply.started":"2023-07-01T08:05:43.57627Z","shell.execute_reply":"2023-07-01T08:05:58.51232Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**В данном нотбуке я покажу, как решить задачу детектирования сорняков для агрокультурного сектора с помощью SOTA модели YOLOv8x**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ultralytics import YOLO\nimport torch\n\nimport shutil\nimport os\n\nimport random\n\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:05:58.516267Z","iopub.execute_input":"2023-07-01T08:05:58.51669Z","iopub.status.idle":"2023-07-01T08:06:05.67823Z","shell.execute_reply.started":"2023-07-01T08:05:58.516652Z","shell.execute_reply":"2023-07-01T08:06:05.677295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"markdown","source":"В качестве данных для обучения взял 2 датасета [WeedCrop Image Dataset](https://www.kaggle.com/datasets/vinayakshanawad/weedcrop-image-dataset) 2822 изображений различного качества и [LincoInBeet](https://datasets.activeloop.ai/docs/ml/datasets/lincolnbeet-dataset/#lincoinbeet-dataset) 4402 высокачественных изображений размером 1920 x 1080 пискелей. \n\nВ сумме у нас получается 7224 изображений, разделим их в соотношении 5558:676:990 соответственно train:val:test.\n\nНо для начала и нужно датасеты собрать и структурировать.","metadata":{}},{"cell_type":"code","source":"source_folder = \"/kaggle/input/weedcrop-image-dataset/WeedCrop.v1i.yolov5pytorch\"\n\n# Папка test\ntest_source_images = os.path.join(source_folder, \"test/images\")\ntest_source_labels = os.path.join(source_folder, \"test/labels\")\n\ntest_destination_folder = \"/kaggle/working/test\"\nos.makedirs(test_destination_folder, exist_ok=True)\n\n# Перемещение изображений\nshutil.copytree(test_source_images, os.path.join(test_destination_folder, \"images\"))\n\n# Перемещение меток\nshutil.copytree(test_source_labels, os.path.join(test_destination_folder, \"labels\"))\n\n\n# Папка train\ntrain_source_images = os.path.join(source_folder, \"train/images\")\ntrain_source_labels = os.path.join(source_folder, \"train/labels\")\n\ntrain_destination_folder = \"/kaggle/working/train\"\nos.makedirs(train_destination_folder, exist_ok=True)\n\n# Перемещение изображений\nshutil.copytree(train_source_images, os.path.join(train_destination_folder, \"images\"))\n\n# Перемещение меток\nshutil.copytree(train_source_labels, os.path.join(train_destination_folder, \"labels\"))\n\n\n# Папка valid\nvalid_source_images = os.path.join(source_folder, \"valid/images\")\nvalid_source_labels = os.path.join(source_folder, \"valid/labels\")\n\nvalid_destination_folder = \"/kaggle/working/valid\"\nos.makedirs(valid_destination_folder, exist_ok=True)\n\n# Перемещение изображений\nshutil.copytree(valid_source_images, os.path.join(valid_destination_folder, \"images\"))\n\n# Перемещение меток\nshutil.copytree(valid_source_labels, os.path.join(valid_destination_folder, \"labels\"))","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:06:05.683125Z","iopub.execute_input":"2023-07-01T08:06:05.685838Z","iopub.status.idle":"2023-07-01T08:06:56.969889Z","shell.execute_reply.started":"2023-07-01T08:06:05.685799Z","shell.execute_reply":"2023-07-01T08:06:56.968905Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def move_files(file_path, source_folder, destination_folder):\n    # Чтение списка названий файлов\n    with open(file_path, \"r\") as file:\n        file_names = [os.path.basename(line.strip()) for line in file.readlines()]\n\n    # Папки для изображений и меток\n    images_folder = os.path.join(destination_folder, \"images\")\n    labels_folder = os.path.join(destination_folder, \"labels\")\n    \n    # Перемещение файлов\n    for file_name in file_names:\n        image_file = os.path.join(source_folder, file_name)\n        label_file = os.path.join(source_folder, file_name.replace(\".png\", \".txt\"))\n        if os.path.isfile(image_file):\n            shutil.copy(image_file, images_folder)\n        if os.path.isfile(label_file):\n            shutil.copy(label_file, labels_folder)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:06:56.97261Z","iopub.execute_input":"2023-07-01T08:06:56.973598Z","iopub.status.idle":"2023-07-01T08:06:56.98192Z","shell.execute_reply.started":"2023-07-01T08:06:56.97356Z","shell.execute_reply":"2023-07-01T08:06:56.980846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_file = \"/kaggle/input/amiran/all_fields_lincolnbeet/all_fields_lincolnbeet_train_.txt\"\nvalid_file = \"/kaggle/input/amiran/all_fields_lincolnbeet/all_fields_lincolnbeet_val_.txt\"\ntest_file = \"/kaggle/input/amiran/all_fields_lincolnbeet/all_fields_lincolnbeet_test_.txt\"\n\nsource_folder = \"/kaggle/input/amiran/all_fields_lincolnbeet/all\"\ntrain_destination = \"/kaggle/working/train\"\nvalid_destination = \"/kaggle/working/valid\"\ntest_destination = \"/kaggle/working/test\"\n\nmove_files(train_file, source_folder, train_destination)\nmove_files(valid_file, source_folder, valid_destination)\nmove_files(test_file, source_folder, test_destination)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:06:56.983734Z","iopub.execute_input":"2023-07-01T08:06:56.984269Z","iopub.status.idle":"2023-07-01T08:12:32.262158Z","shell.execute_reply.started":"2023-07-01T08:06:56.984209Z","shell.execute_reply":"2023-07-01T08:12:32.26107Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Создадим yaml для YOLO\nimport  yaml\n\n# Data structure\ndataset = {\n'train': '/kaggle/working/train',\n'val': '/kaggle/working/valid',\n'test': '/kaggle/working/test',\n'nc': 2,\n'names': ['crop', 'weed']\n}\n\n# save to YAML-file\nwith open('/kaggle/working/dataset.yaml', 'w') as file:\n    yaml.dump(dataset, file)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:12:32.263574Z","iopub.execute_input":"2023-07-01T08:12:32.266228Z","iopub.status.idle":"2023-07-01T08:12:32.273106Z","shell.execute_reply.started":"2023-07-01T08:12:32.266175Z","shell.execute_reply":"2023-07-01T08:12:32.272091Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:12:32.274396Z","iopub.execute_input":"2023-07-01T08:12:32.274801Z","iopub.status.idle":"2023-07-01T08:12:33.237562Z","shell.execute_reply.started":"2023-07-01T08:12:32.27477Z","shell.execute_reply":"2023-07-01T08:12:33.236412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Папки с изображениями и метками\nimages_folder = \"/kaggle/working/train/images\"\nlabels_folder = \"/kaggle/working/train/labels\"\n\n# Загрузка списка файлов изображений\nimage_files = os.listdir(images_folder)\n\n# Выбор случайных изображений\nrandom.shuffle(image_files)\nrandom_image_files = image_files[:6]\n\n# Отображение случайных изображений с метками\nnum_images = len(random_image_files)\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i in range(num_images):\n    # Загрузка изображения\n    image_file = os.path.join(images_folder, random_image_files[i])\n    image = cv2.imread(image_file)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Загрузка меток из файла\n    label_file = os.path.join(labels_folder, os.path.splitext(random_image_files[i])[0] + \".txt\")\n    with open(label_file, \"r\") as file:\n        labels = file.readlines()\n\n    # Отображение изображения с метками\n    for label in labels:\n        class_id, x, y, width, height = map(float, label.strip().split())\n        x = int(x * image.shape[1])\n        y = int(y * image.shape[0])\n        width = int(width * image.shape[1])\n        height = int(height * image.shape[0])\n        cv2.rectangle(image, (x, y), (x + width, y + height), (0, 255, 0), 2)\n\n    # Отображение изображения\n    axes[i].imshow(image)\n    axes[i].axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:12:33.239375Z","iopub.execute_input":"2023-07-01T08:12:33.240105Z","iopub.status.idle":"2023-07-01T08:12:36.177246Z","shell.execute_reply.started":"2023-07-01T08:12:33.240065Z","shell.execute_reply":"2023-07-01T08:12:36.176076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train loop","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:12:36.178561Z","iopub.execute_input":"2023-07-01T08:12:36.178952Z","iopub.status.idle":"2023-07-01T08:12:36.265696Z","shell.execute_reply.started":"2023-07-01T08:12:36.178908Z","shell.execute_reply":"2023-07-01T08:12:36.264583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO('yolov8x.pt')","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:12:36.269336Z","iopub.execute_input":"2023-07-01T08:12:36.270152Z","iopub.status.idle":"2023-07-01T08:12:39.654288Z","shell.execute_reply.started":"2023-07-01T08:12:36.270117Z","shell.execute_reply":"2023-07-01T08:12:39.653259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train(data='/kaggle/working/dataset.yaml ', epochs=50, imgsz=640,\n            optimizer = 'AdamW', lr0 = 1e-3, \n            project = 'TG_YOLOv8x', name='Didi',\n            batch=16, device=device, seed=69)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:12:39.655813Z","iopub.execute_input":"2023-07-01T08:12:39.656162Z","iopub.status.idle":"2023-07-01T16:21:17.229756Z","shell.execute_reply.started":"2023-07-01T08:12:39.656128Z","shell.execute_reply":"2023-07-01T16:21:17.226248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Последние 10 эпох мы уперлись в потолок mAP 0.796","metadata":{}},{"cell_type":"markdown","source":"# Train metrics and losses","metadata":{}},{"cell_type":"markdown","source":"Yolo очень удобный фрэймворк, который сохраняте логи и также готовые графики метрик и лосов.\nНе будем придумывать велосипед и воспользуемся готовым решением.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/TG_YOLOv8x/Didi/results.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:24:07.941667Z","iopub.execute_input":"2023-07-01T16:24:07.942041Z","iopub.status.idle":"2023-07-01T16:24:07.953856Z","shell.execute_reply.started":"2023-07-01T16:24:07.94201Z","shell.execute_reply":"2023-07-01T16:24:07.951248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:24:10.547964Z","iopub.execute_input":"2023-07-01T16:24:10.548369Z","iopub.status.idle":"2023-07-01T16:24:10.559357Z","shell.execute_reply.started":"2023-07-01T16:24:10.548338Z","shell.execute_reply":"2023-07-01T16:24:10.557852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loss","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(15, 15))\nfig.tight_layout()\n\n# train/box_loss\naxes[0, 0].plot(df['                  epoch'], df['         train/box_loss'], label='         train/box_loss')\naxes[0, 0].set_title('Train Box Loss')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\n\n# val/box_loss\naxes[0, 1].plot(df['                  epoch'], df['           val/box_loss'], label='           val/box_loss')\naxes[0, 1].set_title('Validation Box Loss')\naxes[0, 1].set_ylabel('Loss')\naxes[0, 1].legend()\n\n# train/cls_loss\naxes[1, 0].plot(df['                  epoch'], df['         train/cls_loss'], label='         train/cls_loss')\naxes[1, 0].set_title('Train Class Loss')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].legend()\n\n# val/cls_loss\naxes[1, 1].plot(df['                  epoch'], df['           val/cls_loss'], label='           val/cls_loss')\naxes[1, 1].set_title('Validation Class Loss')\naxes[1, 1].set_ylabel('Loss')\naxes[1, 1].legend()\n\n# train/dfl_loss\naxes[2, 0].plot(df['                  epoch'], df['         train/dfl_loss'], label='         train/dfl_loss')\naxes[2, 0].set_title('Train Distribution Focal loss')\naxes[2, 0].set_xlabel('Epoch')\naxes[2, 0].set_ylabel('Loss')\naxes[2, 0].legend()\n\n# val/dfl_loss\naxes[2, 1].plot(df['                  epoch'], df['           val/dfl_loss'], label='           val/dfl_loss')\naxes[2, 1].set_title('Validation Distribution Focal loss')\naxes[2, 1].set_xlabel('Epoch')\naxes[2, 1].set_ylabel('Loss')\naxes[2, 1].legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:24:13.6578Z","iopub.execute_input":"2023-07-01T16:24:13.658182Z","iopub.status.idle":"2023-07-01T16:24:15.164239Z","shell.execute_reply.started":"2023-07-01T16:24:13.658151Z","shell.execute_reply":"2023-07-01T16:24:15.162937Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Metrics на обучении","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.tight_layout()\n\n# metrics/precision(B)\naxes[0, 0].plot(df['                  epoch'], df['   metrics/precision(B)'], label='   metrics/precision(B)')\naxes[0, 0].set_title('Precision')\naxes[0, 0].set_ylabel('Precision')\naxes[0, 0].legend()\n\n# metrics/recall(B)\naxes[0, 1].plot(df['                  epoch'], df['      metrics/recall(B)'], label='      metrics/recall(B)')\naxes[0, 1].set_title('Recall')\naxes[0, 1].set_ylabel('Recall')\naxes[0, 1].legend()\n\n# График для metrics/mAP50(B)\naxes[1, 0].plot(df['                  epoch'], df['       metrics/mAP50(B)'], label='       metrics/mAP50(B)')\naxes[1, 0].set_title('mAP@0.5')\naxes[1, 0].set_ylabel('mAP@0.5')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].legend()\n\n# metrics/mAP50-95(B)\naxes[1, 1].plot(df['                  epoch'], df['    metrics/mAP50-95(B)'], label='    metrics/mAP50-95(B)')\naxes[1, 1].set_title('mAP@0.5:0.95')\naxes[1, 1].set_ylabel('mAP@0.5:0.95')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:24:35.305534Z","iopub.execute_input":"2023-07-01T16:24:35.305919Z","iopub.status.idle":"2023-07-01T16:24:36.325414Z","shell.execute_reply.started":"2023-07-01T16:24:35.305889Z","shell.execute_reply":"2023-07-01T16:24:36.324307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# F1_curve.png\nf1_curve = Image.open(\"/kaggle/working/TG_YOLOv8x/Didi/F1_curve.png\")\nplt.figure(figsize=(10, 10))\nplt.imshow(f1_curve)\nplt.title(\"F1 Curve\")\nplt.axis(\"off\")\nplt.show()\n\n# PR_curve.png\npr_curve = Image.open(\"/kaggle/working/TG_YOLOv8x/Didi/PR_curve.png\")\nplt.figure(figsize=(10, 10))\nplt.imshow(pr_curve)\nplt.title(\"Precision-Recall Curve\")\nplt.axis(\"off\")\nplt.show()\n\n# P_curve.png\np_curve = Image.open(\"/kaggle/working/TG_YOLOv8x/Didi/P_curve.png\")\nplt.figure(figsize=(10, 10))\nplt.imshow(p_curve)\nplt.title(\"Precision Curve\")\nplt.axis(\"off\")\nplt.show()\n\n# R_curve.png\nr_curve = Image.open(\"/kaggle/working/TG_YOLOv8x/Didi/R_curve.png\")\nplt.figure(figsize=(10, 10))\nplt.imshow(r_curve)\nplt.title(\"Recall Curve\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:25:21.838384Z","iopub.execute_input":"2023-07-01T16:25:21.838754Z","iopub.status.idle":"2023-07-01T16:25:24.123557Z","shell.execute_reply.started":"2023-07-01T16:25:21.838724Z","shell.execute_reply":"2023-07-01T16:25:24.12264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Как видно по precision-recall curve графику, crop ,т.е. посевы имеют высокий процент детекции моделью, а вот с сорняками дела обстоят не так хорошо. Нужно больше данных с помеченными сорянками.","metadata":{}},{"cell_type":"code","source":"# confusion matrix\nconfusion_matrix = Image.open(\"/kaggle/working/TG_YOLOv8x/Didi/confusion_matrix.png\")\nplt.figure(figsize=(8, 6))\nplt.imshow(confusion_matrix)\nplt.title(\"Confusion Matrix\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:25:45.957394Z","iopub.execute_input":"2023-07-01T16:25:45.957995Z","iopub.status.idle":"2023-07-01T16:25:46.853463Z","shell.execute_reply.started":"2023-07-01T16:25:45.957964Z","shell.execute_reply":"2023-07-01T16:25:46.852248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Наибольшее количество ошибок нашей модели было связано с неправильной классификацией объектов как \"background\", в то время как на самом деле они были сорняками - 903 таких ошибок.И наоборот,  произошло 955 ошибок, когда модель классифицировала сорняки как \"background\".\n\nВ остальных случаях количество ошибок было незначительным, в районе значений 100-200.","metadata":{}},{"cell_type":"markdown","source":"# Evaluation ","metadata":{}},{"cell_type":"markdown","source":"Посмотрим как модель детектирует на тестовом наборе","metadata":{}},{"cell_type":"code","source":"res = model('/kaggle/working/test/images/bbro_bbro_14_05_2021_v_0_18.png')\ndetect_img = res[0].plot()\ndetect_img = cv2.cvtColor(detect_img, cv2.COLOR_BGR2RGB)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Отображение первого изображения\naxes[0].imshow(plt.imread('/kaggle/working/test/images/bbro_bbro_14_05_2021_v_0_18.png'))\naxes[0].axis('off')\n\n# Отображение результатов модели\naxes[1].imshow(detect_img)\naxes[1].axis('off')\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:29:58.815538Z","iopub.execute_input":"2023-07-01T16:29:58.815928Z","iopub.status.idle":"2023-07-01T16:29:59.801816Z","shell.execute_reply.started":"2023-07-01T16:29:58.8159Z","shell.execute_reply":"2023-07-01T16:29:59.800597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Сохраним веса модели на обучении","metadata":{}},{"cell_type":"code","source":"model = YOLO('TG_YOLOv8x/Didi/weights/best.pt ')","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:27:01.695969Z","iopub.execute_input":"2023-07-01T16:27:01.696384Z","iopub.status.idle":"2023-07-01T16:27:01.935238Z","shell.execute_reply.started":"2023-07-01T16:27:01.69635Z","shell.execute_reply":"2023-07-01T16:27:01.934133Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = model.val(split='test', conf=0.25, device=device) # conf - это порог достоверности объекта для обнаружения","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:27:04.957159Z","iopub.execute_input":"2023-07-01T16:27:04.958051Z","iopub.status.idle":"2023-07-01T16:28:43.702059Z","shell.execute_reply.started":"2023-07-01T16:27:04.958018Z","shell.execute_reply":"2023-07-01T16:28:43.700426Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:28:43.708849Z","iopub.execute_input":"2023-07-01T16:28:43.711661Z","iopub.status.idle":"2023-07-01T16:28:43.743043Z","shell.execute_reply.started":"2023-07-01T16:28:43.711613Z","shell.execute_reply":"2023-07-01T16:28:43.737492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = sns.barplot(x=['mAP50-95', 'mAP50', 'mAP75'], y=[metrics.box.map, metrics.box.map50, metrics.box.map75])\n\n\nax.set_title('YOLO Evaluation Metrics')\nax.set_xlabel('Metric')\nax.set_ylabel('Value')\n\n\nfig = plt.gcf()\nfig.set_size_inches(8, 6)\n\nfor p in ax.patches:\n    ax.annotate('{:.3f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n    \n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:30:13.976288Z","iopub.execute_input":"2023-07-01T16:30:13.97668Z","iopub.status.idle":"2023-07-01T16:30:14.280365Z","shell.execute_reply.started":"2023-07-01T16:30:13.976651Z","shell.execute_reply":"2023-07-01T16:30:14.279278Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Таким образом, модель демонстрирует более высокую точность в обнаружении объектов при более низких порогах уверенности, а при более высоких порогах её точность снижается. Это может быть связано с тем, что при высоких порогах уверенности модель становится более консервативной и пропускает некоторые объекты, чтобы снизить вероятность ложных срабатываний.","metadata":{}},{"cell_type":"code","source":"# Извлечение значений Precision, Recall и F1\nprecision = metrics.results_dict['metrics/precision(B)']\nrecall = metrics.results_dict['metrics/recall(B)']\nf1 = (2 * precision * recall) / (precision + recall)  # Вычисление F1\n\n\nmetrics = ['Precision', 'Recall', 'F1']\nvalues = [precision, recall, f1]\n\n# Создание графика с использованием sns.barplot\nax = sns.barplot(x=metrics, y=values, palette='viridis')\n\nax.set_title('Precision, Recall, and F1 Scores')\nax.set_xlabel('Metric')\nax.set_ylabel('Value')\n\nfor p in ax.patches:\n    ax.annotate('{:.3f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:30:19.079554Z","iopub.execute_input":"2023-07-01T16:30:19.079946Z","iopub.status.idle":"2023-07-01T16:30:19.349307Z","shell.execute_reply.started":"2023-07-01T16:30:19.079917Z","shell.execute_reply":"2023-07-01T16:30:19.34833Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Как можно убедиться из метрик, 'Precision' модели очень даже неплохая, она распознает хорошо оба класса и находит их с примерно с одинаковым показателем.","metadata":{}},{"cell_type":"markdown","source":"Взглянем на 10 случайных изображений ","metadata":{}},{"cell_type":"code","source":"# Получение случайных изображений\nimage_paths = random.sample(os.listdir(images_folder), 10)\n\n# Создание фигуры с подокнами и увеличенным размером фотографий\nfig, axes = plt.subplots(2, 5, figsize=(20, 12))\nfig.tight_layout()\n\n# Итерация по каждому подокну\nfor i, ax in enumerate(axes.flat):\n    image_path = os.path.join(images_folder, image_paths[i])\n    image = Image.open(image_path)\n    res = model(image, verbose=False)\n    detect_img = res[0].plot()\n    detect_img = cv2.cvtColor(detect_img, cv2.COLOR_BGR2RGB)\n    \n    ax.imshow(detect_img)\n    ax.axis('off')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:30:24.767646Z","iopub.execute_input":"2023-07-01T16:30:24.768025Z","iopub.status.idle":"2023-07-01T16:30:28.251554Z","shell.execute_reply.started":"2023-07-01T16:30:24.767994Z","shell.execute_reply":"2023-07-01T16:30:28.25028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Вывод","metadata":{}},{"cell_type":"markdown","source":"\nВ этом ноутбуке мы обучили модель YOLOv8x на объединенных данных, изучили метрики и потери в процессе обучения, а также проанализировали вывод модели на нескольких изображениях и качество детекции на тестовых данных.\n\nДля достижения более высоких показателей модели и успешного применения ее в бизнес-процессах рекомендуется провести более тщательные исследования. Вот некоторые шаги, которые могут быть предприняты:\n\nСобрать больше данных: Больший объем разнообразных данных поможет модели лучше обобщать и распознавать объекты. Можно провести сбор новых данных или найти доступные датасеты для дополнения текущего обучающего набора.\n\nНастроить гиперпараметры: Подбор оптимальных гиперпараметров модели может значительно повлиять на ее производительность. Можно провести эксперименты с различными значениями гиперпараметров, такими как размеры якорей, пороги уверенности и другие, чтобы достичь лучших результатов.\n\nПровести дополнительные эксперименты: Экспериментирование с различными архитектурами модели, вариациями YOLOv8 или других моделей детекции объектов, может привести к лучшим результатам. \n\nи т.д.\n\nВ целом, более точные исследования (в  частности нужно больше помеченных данных с метками и боксами сорняков), эксперименты и работа над улучшением модели позволят достичь более высоких показателей и повысить ее применимость в реальных бизнес-сценариях.\nКонечно же все упирается в ресурсы, как вычислительные, так и денежные.","metadata":{}}]}